!pip install datasets
from transformers import pipeline, AutoTokenizer
from tensorflow.keras.optimizers import Adam # Import Adam
import tensorflow as tf
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, GRU, Bidirectional
from transformers import BertTokenizer, TFBertForSequenceClassification, RobertaTokenizer, TFRobertaForSequenceClassification, AlbertTokenizer, TFAlbertForSequenceClassification, AdamWeightDecay
from sklearn.metrics import accuracy_score, confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
from datasets import load_dataset
import matplotlib.pyplot as plt
import seaborn as sns

# Define MAX_VOCAB_SIZE and MAX_SEQUENCE_LENGTH here
MAX_VOCAB_SIZE = 10000  # Vocabulary size
MAX_SEQUENCE_LENGTH = 500  # Max review length

# Load and preprocess the data for the LSTM model
# (This was likely missing before causing the NameError)
# Explicitly set `download_mode` to force download
dataset = load_dataset("imdb", download_mode="force_redownload")
train_data = dataset["train"]
test_data = dataset["test"]

tokenizer_lstm = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<OOV>")
tokenizer_lstm.fit_on_texts(train_data["text"])

X_train = pad_sequences(tokenizer_lstm.texts_to_sequences(train_data["text"]), maxlen=MAX_SEQUENCE_LENGTH, padding="post")
X_test = pad_sequences(tokenizer_lstm.texts_to_sequences(test_data["text"]), maxlen=MAX_SEQUENCE_LENGTH, padding="post")

y_train = np.array(train_data["label"])
y_test = np.array(test_data["label"])

# Tokenizer for LSTM model
MAX_VOCAB_SIZE = 10000  # Vocabulary size
MAX_SEQUENCE_LENGTH = 500  # Max review length

print("\nPreprocessing data for LSTM model...")
tokenizer_lstm = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<OOV>")
tokenizer_lstm.fit_on_texts(train_data["text"])

# Convert text to sequences and pad them
X_train = pad_sequences(tokenizer_lstm.texts_to_sequences(train_data["text"]), maxlen=MAX_SEQUENCE_LENGTH, padding="post")
X_test = pad_sequences(tokenizer_lstm.texts_to_sequences(test_data["text"]), maxlen=MAX_SEQUENCE_LENGTH, padding="post")

# Convert labels to numpy arrays
y_train = np.array(train_data["label"])
y_test = np.array(test_data["label"])

# Define LSTM model
model_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=128, input_length=MAX_SEQUENCE_LENGTH),
    tf.keras.layers.LSTM(128, return_sequences=True),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model_lstm.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
print("\nTraining LSTM model...")
model_lstm.fit(X_train, y_train, validation_data=(X_test[:5000], y_test[:5000]), epochs=3, batch_size=128)

# Evaluate LSTM model
print("\nEvaluating LSTM model...")
y_pred_probs = model_lstm.predict(X_test[:1500])
y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_probs]

# Confusion Matrix for LSTM
cm_lstm = confusion_matrix(y_test[:1500], y_pred)
print("\nConfusion Matrix for LSTM:")
print(cm_lstm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_lstm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Negative", "Positive"],
            yticklabels=["Negative", "Positive"])
plt.title("LSTM Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

accuracy_lstm = accuracy_score(y_test[:1500], y_pred)
print(f"LSTM Model Accuracy (on 1500 samples): {accuracy_lstm * 100:.2f}%")

# Add this code before using sample_reviews
sample_reviews = [
    "This movie was amazing! I loved it.",
    "The worst film I've ever seen. Absolutely terrible.",
    "It was okay, but nothing special.",
    "A fantastic and captivating story.",
    "I would not recommend this to anyone."
]


# Test LSTM model on sample reviews
sample_sequences = pad_sequences(tokenizer_lstm.texts_to_sequences(sample_reviews), maxlen=MAX_SEQUENCE_LENGTH, padding="post")
sample_preds = model_lstm.predict(sample_sequences)

print("\nTesting sample reviews with LSTM:")
for review, pred in zip(sample_reviews, sample_preds):
    sentiment = "POSITIVE" if pred > 0.5 else "NEGATIVE"
    print(f"\nReview: {review}")
    print(f"Prediction: {sentiment}, Confidence: {pred[0]:.2f}")

# Define CNN-LSTM Model
print("\nBuilding CNN-LSTM model...")
model_cnn_lstm = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=128, input_length=MAX_SEQUENCE_LENGTH),
    Conv1D(filters=64, kernel_size=5, activation='relu'),
    # Remove GlobalMaxPooling1D or replace it with a layer that maintains 3D output
    # Example: tf.keras.layers.MaxPooling1D(pool_size=2)
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model_cnn_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train CNN-LSTM Model
print("\nTraining CNN-LSTM model...")
model_cnn_lstm.fit(X_train, y_train, validation_data=(X_test[:5000], y_test[:5000]), epochs=3, batch_size=128)

# Evaluate CNN-LSTM Model
y_pred_probs_cnn_lstm = model_cnn_lstm.predict(X_test[:1500])
y_pred_cnn_lstm = [1 if prob > 0.5 else 0 for prob in y_pred_probs_cnn_lstm]

# Confusion Matrix for CNN-LSTM
cm_cnn_lstm = confusion_matrix(y_test[:1500], y_pred_cnn_lstm)
print("\nConfusion Matrix for CNN-LSTM:")
print(cm_cnn_lstm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_cnn_lstm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Negative", "Positive"],
            yticklabels=["Negative", "Positive"])
plt.title("CNN-LSTM Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

accuracy_cnn_lstm = accuracy_score(y_test[:1500], y_pred_cnn_lstm)
print(f"CNN-LSTM Model Accuracy: {accuracy_cnn_lstm * 100:.2f}%")

# Define BiGRU Model
print("\nBuilding BiGRU model...")
model_bigru = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=128, input_length=MAX_SEQUENCE_LENGTH),
    Bidirectional(GRU(64, return_sequences=True)),
    Bidirectional(GRU(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model_bigru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train BiGRU Model
print("\nTraining BiGRU model...")
model_bigru.fit(X_train, y_train, validation_data=(X_test[:5000], y_test[:5000]), epochs=3, batch_size=128)

# Evaluate BiGRU Model
y_pred_probs_bigru = model_bigru.predict(X_test[:1500])
y_pred_bigru = [1 if prob > 0.5 else 0 for prob in y_pred_probs_bigru]

# Confusion Matrix for BiGRU
cm_bigru = confusion_matrix(y_test[:1500], y_pred_bigru)
print("\nConfusion Matrix for BiGRU:")
print(cm_bigru)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_bigru, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Negative", "Positive"],
            yticklabels=["Negative", "Positive"])
plt.title("BiGRU Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

accuracy_bigru = accuracy_score(y_test[:1500], y_pred_bigru)
print(f"BiGRU Model Accuracy: {accuracy_bigru * 100:.2f}%")

# Initialize DistilBERT sentiment analysis pipeline
print("Loading DistilBERT model...")
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
classifier = pipeline("sentiment-analysis", model=model_name)
tokenizer_bert = AutoTokenizer.from_pretrained(model_name)

# Test DistilBERT on sample reviews
sample_reviews = [
    "This movie was terrible! Waste of time.",
    "I loved every minute of it. Brilliant!",
    "Meh, it was okay. Not great, not awful."
]

print("\nTesting sample reviews with DistilBERT:")
for review in sample_reviews:
    result = classifier(review)[0]
    print(f"\nReview: {review}")
    print(f"Label: {result['label']}, Confidence: {result['score']:.2f}")

# Evaluate DistilBERT on a test subset
print("\nEvaluating DistilBERT on test data...")
test_samples = dataset["test"].select(range(1500))
predictions = classifier(test_samples["text"], truncation=True)
true_labels = ["POSITIVE" if label == 1 else "NEGATIVE" for label in test_samples["label"]]
pred_labels = [pred["label"] for pred in predictions]

# Confusion Matrix for DistilBERT
cm_bert = confusion_matrix(true_labels, pred_labels)
print("\nConfusion Matrix for DistilBERT:")
print(cm_bert)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_bert, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Negative", "Positive"],
            yticklabels=["Negative", "Positive"])
plt.title("DistilBERT Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

accuracy_bert = accuracy_score(true_labels, pred_labels)
print(f"\nDistilBERT Model Accuracy (on 1500 samples): {accuracy_bert * 100:.2f}%")

# Final Comparison
print("\nFinal Comparison of Models:")
print(f"LSTM Accuracy: {accuracy_lstm * 100:.2f}%") # accuracy_lstm may be undefined. You might want to evaluate your original LSTM model here again or pass it as an argument.
print(f"CNN-LSTM Accuracy: {accuracy_cnn_lstm * 100:.2f}%")
print(f"BiGRU Accuracy: {accuracy_bigru * 100:.2f}%")
print(f"DistilBERT Accuracy: {accuracy_bert * 100:.2f}%")
